# -*- coding: utf-8 -*-
"""Car_price.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19z3TK5NO-Bs6hii2uovdu8pV3omXJnxc

Loading the data from cloud
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

df=pd.read_csv('/content/drive/MyDrive/CarPrice_Assignment.csv')
df.shape

df.head()

df.info()

df.describe()

#Splitting company name from CarName column
CompanyName = df['CarName'].apply(lambda x : x.split(' ')[0])
df.insert(3,"CompanyName",CompanyName)
df.drop(['CarName'],axis=1,inplace=True)
df.head()

df.CompanyName = df.CompanyName.str.lower()

def replace_name(a,b):
    df.CompanyName.replace(a,b,inplace=True)

replace_name('maxda','mazda')
replace_name('porcshce','porsche')
replace_name('toyouta','toyota')
replace_name('vokswagen','volkswagen')
replace_name('vw','volkswagen')

print(df.CompanyName.unique())
df.CompanyName.value_counts()

"""# Exploratory data analysis
Histogram analysis for numerical variables
"""

#Histogram for car price
import matplotlib.pyplot as plt
import seaborn as sns

sns.distplot(df.price)

sns.distplot(df.horsepower)

"""Barplot analysis for categorical variables"""

sns.barplot(data=df,x='carbody',y='price')

sns.barplot(data=df,x='drivewheel',y='price')

sns.barplot(data=df,x='fuelsystem',y='price')

sns.barplot(data=df,y='CompanyName',x='price',orient='h')

plt01 = df.CompanyName.value_counts().plot(kind='bar')
plt.title('Companies Histogram')
plt01.set(xlabel = 'Car company', ylabel='Frequency of company')

sns.barplot(data=df,x='fueltype',y='price')

"""Correlation analysis using heatmap"""

corr = df.corr()
plt.figure(figsize=(12, 9))
sns.heatmap(corr, annot = True)

selected_cols=df[['wheelbase','carlength','carwidth','carheight','curbweight','enginesize','boreratio','stroke','compressionratio','horsepower','peakrpm','citympg','highwaympg','price']]

#sns.pairplot(selected_cols)

"""Checking multicollinearity using VIF"""

#Defining model and checking multicollinearity of variables
from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.api as sm
def build_model(X,y):
    X = sm.add_constant(X) #Adding the constant
    lm = sm.OLS(y,X).fit() # fitting the model
    print(lm.summary()) # model summary
    return X
def checkVIF(X):
    vif = pd.DataFrame()
    vif['Features'] = X.columns
    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    vif['VIF'] = round(vif['VIF'], 2)
    vif = vif.sort_values(by = "VIF", ascending = False)
    return(vif)

checkVIF(selected_cols)

#Dividing data into X and y variables
from sklearn.model_selection import train_test_split
y = selected_cols.pop('price')
X = selected_cols
X_train, X_test, y_train, y_test = train_test_split(X,y , random_state=104, train_size=0.8, shuffle=True)

#Model1
md1=build_model(X_train,y_train)

checkVIF(md1)

"""Feature engineering"""

#Dropping the columns with high multicollinearity
X_train_2 = X_train.drop(['citympg','highwaympg','curbweight'],axis=1)

#Model2
md2=build_model(X_train_2,y_train)
checkVIF(md2)

X_train_3 =X_train_2.drop(['carlength'],axis=1)

#Model3
md3=build_model(X_train_3,y_train)
checkVIF(md3)

"""Elimination based on multicollinearity,p-value and the correlation"""

#Dropping the columns with high p-value
X_train_4 = X_train_3.drop(['wheelbase','carheight','boreratio'],axis=1)

md4=build_model(X_train_4,y_train)
checkVIF(md4)

col=X_train_4.columns
X_test_1=X_test[['carwidth', 'enginesize', 'stroke', 'compressionratio', 'horsepower','peakrpm']]

# Predict using the trained model on the test set
# Making predictions
from sklearn.linear_model import LinearRegression
lm = LinearRegression()
fitted=lm.fit(X_train_4,y_train)
y_pred = lm.predict(X_test_1)


# You can also calculate metrics to evaluate the predictions, for example, Mean Squared Error
from sklearn.metrics import mean_squared_error

mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

from sklearn.metrics import r2_score
r2_score(y_test, y_pred)

# Compute residuals
residuals = y_test - y_pred

# Compute Mean Squared Error (MSE) to assess model fit
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")

# Plot residuals vs. predicted values
plt.scatter(y_pred, residuals)
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")
plt.title("Residuals vs. Predicted Values")
plt.show()

import statsmodels.api as sm
import scipy.stats as stats

model = sm.OLS(y_train, X_train_4).fit()

# Get the residuals from the model
residuals = model.resid

# Create a Normal Q-Q plot for the residuals
plt.figure(figsize=(8, 6))
stats.probplot(residuals, dist="norm", plot=plt)
plt.title("Normal Q-Q Plot for Residuals")
plt.xlabel("Theoretical Quantiles")
plt.ylabel("Sample Quantiles")
plt.grid(True)
plt.show()

import numpy as np
user_input = []
for col in range(X_train_4.shape[1]):  # Iterate over the number of columns
    value = float(input(f"Enter value for {X_train_4.columns[col]}: "))
    user_input.append(value)

# Predict using the trained model
user_input_array = np.array(user_input).reshape(1, -1)  # Reshape for prediction
user_input_array_with_constant = sm.add_constant(user_input_array)  # Add constant for intercept
predicted_value = model.predict(user_input_array_with_constant)

print("Predicted car price:", predicted_value[0])

!jupyter nbconvert --to html /content/drive/MyDrive/ColabNotebooks/Car_price.ipynb